{
  "submissions": {
    "d55426f5-849b-492b-9a6e-6cc1f28fe606": {
      "assignment_id": "2d2de4cd-7017-48e3-a083-f1e2a0e8aef3",
      "student_id": "691f23ca7c94751f7f688d81",
      "file": {
        "filename": "lab9.ipynb",
        "file_path": "storage\\submissions\\2d2de4cd-7017-48e3-a083-f1e2a0e8aef3\\691f23ca7c94751f7f688d81\\283a2790-3387-450a-a205-78a71c3039bb_lab9.ipynb",
        "file_size": 77950,
        "content_type": "application/json",
        "uploaded_at": "2025-11-20T23:03:41.297177+00:00"
      },
      "status": "graded",
      "is_late": false,
      "id": "d55426f5-849b-492b-9a6e-6cc1f28fe606",
      "submitted_at": "2025-11-20T23:03:41.297238+00:00",
      "grade": {
        "score": 40.0,
        "feedback": "**Overall impression**\n\nYou\u2019ve got a solid start: the notebook loads the data, cleans missing values, encodes categorical variables, scales the features, and trains an SVM. The code is readable and follows a logical flow. However, there are a few gaps that, if addressed, will make the notebook more robust, reproducible, and easier to maintain. Below is a detailed, constructive review that covers correctness, structure, style, and areas for improvement.\n\n---\n\n## 1. Code correctness & logic\n\n| \u2705 \u2714 | \u2705 \u2714 | \u274c \u274c | \ud83d\udd0d Notes |\n|------|------|-------|----------|\n| **Data loading** | \u2714 |  | `pd.read_csv('Loan.csv')` works if the file is in the working directory. |\n| **Missing\u2011value handling** | \u2714 |  | You correctly impute numerics with the median and categoricals with the mode. |\n| **Target separation** | \u2714 |  | `X` and `Y` are defined correctly. |\n| **Label encoding** | \u274c |  | Using a single `LabelEncoder` instance for *all* categorical columns will overwrite the mapping for each column. This means that the encoding of the first column will be lost when you encode the second one. Use a separate encoder per column or a `ColumnTransformer`. |\n| **Scaling** | \u2714 |  | `StandardScaler` is fine for SVMs, but remember that it should be fitted only on the training data. You did that correctly. |\n| **Train/test split** | \u2714 |  | You used `stratify=Y`, which is good for imbalanced targets. |\n| **Model training** | \u2714 |  | `SVC(kernel='rbf', random_state=42)` is fine. |\n| **Parameter printing** | \u2714 |  | You correctly print `C` and `gamma`. |\n| **Model evaluation** | \u274c |  | The notebook stops after printing parameters. You never predict on the test set or compute any metrics (accuracy, confusion matrix, etc.). |\n| **Handling class imbalance** | \u274c |  | If `loan_status` is imbalanced, consider `class_weight='balanced'` or resampling. |\n| **Reproducibility** | \u274c |  | You set `random_state` for split and SVC, but you should also set a global seed for NumPy and any other stochastic processes. |\n\n### Suggested fixes\n\n```python\n# Separate encoders for each categorical column\nencoders = {}\nfor col in X.select_dtypes('object').columns:\n    encoders[col] = LabelEncoder()\n    X[col] = encoders[col].fit_transform(X[col])\n\n# Encode target separately\ny_encoder = LabelEncoder()\nY = y_encoder.fit_transform(Y)\n```\n\n```python\n# After training\nY_pred = svm_model.predict(X_test)\n\nprint(\"Accuracy:\", accuracy_score(Y_test, Y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(Y_test, Y_pred))\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(Y_test, Y_pred))\n```\n\n---\n\n## 2. Code structure & organization\n\n| \u2705 \u2714 | \u274c \u274c | \ud83d\udd0d Notes |\n|------|------|----------|\n| **Modularization** | \u274c | All code runs at the top level. Wrap repetitive tasks (e.g., preprocessing, model training, evaluation) into functions or a class. |\n| **Use of `__main__` guard** | \u274c | Not needed in a notebook, but if you export to a script, add `if __name__ == \"__main__\":`. |\n| **Pipeline** | \u274c | A `Pipeline` (or `ColumnTransformer` + `Pipeline`) would keep preprocessing and modeling together, making it easier to tune hyper\u2011parameters later. |\n| **Documentation** | \u274c | Add docstrings to functions and brief comments explaining *why* you do something (e.g., why you choose median imputation). |\n| **Error handling** | \u274c | Wrap file loading in a try/except to give a clear message if the CSV is missing. |\n| **Reproducibility** | \u274c | Set a global random seed: `np.random.seed(42)`. |\n\n### Suggested structure\n\n```python\ndef load_data(path: str) -> pd.DataFrame:\n    ...\n\ndef preprocess(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, dict]:\n    ...\n\ndef build_pipeline() -> Pipeline:\n    ...\n\ndef evaluate(model, X_test, y_test):\n    ...\n\nif __name__ == \"__main__\":\n    df = load_data('Loan.csv')\n    X, y, encoders = preprocess(df)\n    X_train, X_test, y_train, y_test = train_test_split(...)\n    model = build_pipeline().fit(X_train, y_train)\n    evaluate(model, X_test, y_test)\n```\n\n---\n\n## 3. Best practices & style\n\n| \u2705 \u2714 | \u274c \u274c | \ud83d\udd0d Notes |\n|------|------|----------|\n| **Imports** | \u2714 | All necessary libraries are imported. |\n| **PEP\u20118** | \u274c | Variable names like `X_train` are fine, but avoid single\u2011letter names (`X`, `Y`). Use `X_features`, `y_target`. |\n| **Magic numbers** | \u274c | Hard\u2011coded `0.2` for test size and `42` for random state. Consider defining constants at the top (`TEST_SIZE = 0.2`). |\n| **Printing** | \u274c | Use `print(f\"Missing values after handling:\\n{df.isnull().sum()}\")` for cleaner output. |\n| **Suppress warnings** | \u274c | Add `import warnings; warnings.filterwarnings('ignore')` if you want a cleaner notebook. |\n| **Plotting** | \u274c | You imported seaborn and matplotlib but never used them. If you plan to visualize, add a quick plot (e.g., class distribution). |\n| **Documentation** | \u274c | Add a brief header comment explaining the goal of the notebook. |\n| **Version control** | \u274c | Mention the versions of key libraries (`pd.__version__`, `sklearn.__version__`). |\n\n---\n\n## 4. Areas for improvement\n\n1. **Complete the evaluation** \u2013 Add predictions, compute accuracy, precision, recall, F1, and display a confusion matrix.  \n2. **Use a Pipeline** \u2013 Combine `StandardScaler`, `LabelEncoder` (via `OneHotEncoder` or `OrdinalEncoder`), and `SVC` into a single pipeline.  \n3. **Hyper\u2011parameter tuning** \u2013 Use `GridSearchCV` or `RandomizedSearchCV` to find the best `C` and `gamma`.  \n4. **Handle class imbalance** \u2013 If the dataset is imbalanced, try `class_weight='balanced'` or resampling techniques.  \n5. **Feature engineering** \u2013 Explore adding interaction terms or dropping highly correlated features.  \n6. **Cross\u2011validation** \u2013 Instead of a single train/test split, use `cross_val_score` to get a more reliable estimate.  \n7. **Reproducibility** \u2013 Set a global random seed and document library versions.  \n8. **Code modularity** \u2013 Break the notebook into cells that each perform a single logical step and add function definitions.  \n9. **Visualization** \u2013 Add plots:  \n   - Class distribution before/after encoding.  \n   - Feature importance (e.g., using `coef_` from linear models or SHAP values).  \n10. **Documentation** \u2013 Add a README or markdown cells explaining the dataset, the goal, and the steps taken.\n\n---\n\n## Encouragement & next steps\n\nYou\u2019ve already tackled the core data\u2011science workflow: loading, cleaning, encoding, scaling, splitting, and training. That\u2019s a solid foundation! The next few steps\u2014completing the evaluation, structuring the code into reusable components, and exploring hyper\u2011parameter tuning\u2014will elevate this notebook from a good exercise to a production\u2011ready script.\n\nKeep experimenting with different preprocessing strategies (e.g., one\u2011hot encoding vs. ordinal encoding) and model choices (e.g., RandomForest, XGBoost). And remember: a clean, modular codebase not only makes debugging easier but also showcases your professionalism to future employers or collaborators.\n\nGood job so far\u2014keep up the great work!",
        "graded_at": "2025-11-20T23:04:23.736926+00:00",
        "graded_by": "ai",
        "test_results": {
          "tests_run": false,
          "message": "No test cases available for this assignment"
        }
      }
    },
    "808152e8-86b0-4926-b179-24fe3edabc6b": {
      "assignment_id": "e8414f02-c0d4-43e7-870f-109482d29fa2",
      "student_id": "691f23ca7c94751f7f688d81",
      "file": {
        "filename": "diabetes_logistic_regression.ipynb",
        "file_path": "storage\\submissions\\e8414f02-c0d4-43e7-870f-109482d29fa2\\691f23ca7c94751f7f688d81\\2370c2ed-0c1c-4a5b-a877-f6433132d75c_diabetes_logistic_regression.ipynb",
        "file_size": 167339,
        "content_type": "application/json",
        "uploaded_at": "2025-11-20T23:15:43.240620+00:00"
      },
      "status": "graded",
      "is_late": false,
      "id": "808152e8-86b0-4926-b179-24fe3edabc6b",
      "submitted_at": "2025-11-20T23:15:43.241091+00:00",
      "grade": {
        "score": 95.0,
        "feedback": "**Overall impression**  \nYou\u2019ve built a solid end\u2011to\u2011end pipeline that loads the data, cleans it a bit, visualises it, trains a logistic regression model, and reports the key metrics. The notebook runs without errors and produces the expected plots and numbers. Great job getting everything to work! Below are some targeted suggestions that will help you tighten the code, make it more robust, and follow best practices that are common in production\u2011grade notebooks.\n\n---\n\n## 1. Code correctness & logic\n\n| \u2705 \u2714\ufe0f | \u2705 \u2714\ufe0f | \u274c \u274c |\n|-------|-------|-------|\n| **Data loading** \u2013 `pd.read_csv('diabetes.csv')` works, but consider adding `error_bad_lines=False` or `on_bad_lines='skip'` to guard against malformed rows. | **Feature split** \u2013 You correctly separate `X` and `y`. | **Missing\u2011value handling** \u2013 The dataset contains zeros in columns like `Insulin` and `BMI` that actually represent missing values. You should replace those zeros with `NaN` and impute (e.g., median) before scaling. |\n| **Train/test split** \u2013 Good that you split before scaling. | **Stratification** \u2013 Add `stratify=y` to `train_test_split` to preserve class proportions. | **LogisticRegression defaults** \u2013 The default solver (`lbfgs`) may converge slowly on this dataset. Set `max_iter=1000` (or more) and `random_state=42` for reproducibility. |\n| **Scatterplot** \u2013 Using `sns.scatterplot` with `hue=y_train` works, but you\u2019re passing `data=X_train.assign(Outcome=y_train)`. It\u2019s clearer to create a single DataFrame: `plot_df = X_train.copy(); plot_df['Outcome'] = y_train`. | **Confusion matrix** \u2013 You plot the matrix correctly. Consider adding `cbar=False` and `annot_kws={'size': 12}` for readability. | **Classification report** \u2013 The `target_names` order should match the class order (`0` \u2192 \u201cwithout diabetes\u201d, `1` \u2192 \u201cwith diabetes\u201d). Double\u2011check that the mapping is correct. |\n| **Metrics** \u2013 Accuracy and F1\u2011score are computed correctly. | | |\n\n---\n\n## 2. Code structure & organization\n\n| \u2705 \u2714\ufe0f | \u274c \u274c |\n|-------|-------|\n| **Modular sections** \u2013 You\u2019ve broken the notebook into numbered sections, which is great for readability. | **Repetition** \u2013 The `import` block is split across the file. Keep all imports together at the top. |\n| **Variable naming** \u2013 `df`, `X`, `y`, `X_train`, etc., are clear. | **Hard\u2011coded file path** \u2013 Use a relative path or a `Path` object (`from pathlib import Path`) to make the notebook portable. |\n| **Plotting** \u2013 Each plot is shown with `plt.show()`. | **Missing `plt.tight_layout()`** \u2013 Add this after each figure to avoid label overlap. |\n| **Notebook cells** \u2013 Each logical block is in its own cell, which is good for incremental execution. | **No `if __name__ == \"__main__\"`** \u2013 Not required in a notebook, but if you plan to convert to a script, wrap the main logic in a function. |\n\n---\n\n## 3. Best practices & style\n\n| \u2705 \u2714\ufe0f | \u274c \u274c |\n|-------|-------|\n| **PEP\u20118** \u2013 Variable names and spacing are mostly compliant. | **Missing docstrings** \u2013 Add a brief description at the top of the notebook or in a markdown cell explaining the goal. |\n| **Imports** \u2013 All necessary libraries are imported. | **Unused imports** \u2013 `seaborn as sns` is used, but you never set a style. Add `sns.set_style('whitegrid')` or `sns.set_context('notebook')`. |\n| **Reproducibility** \u2013 You set `random_state=42` in `train_test_split`. | **Random state for model** \u2013 Pass `random_state=42` to `LogisticRegression`. |\n| **Pipeline** \u2013 You manually scale and fit. | **Pipeline** \u2013 Use `sklearn.pipeline.Pipeline` to chain `StandardScaler` and `LogisticRegression`. This ensures the same preprocessing is applied during cross\u2011validation and makes the code cleaner. |\n| **Error handling** \u2013 None needed for this simple script. | **Missing `warnings.filterwarnings('ignore')`** \u2013 If you want to suppress convergence warnings, add this. |\n| **Plot aesthetics** \u2013 Good use of `palette='viridis'`. | **Figure size** \u2013 Consider setting a global figure size with `plt.rcParams['figure.figsize'] = (8,6)` to keep consistency. |\n\n---\n\n## 4. Areas for improvement\n\n1. **Missing\u2011value treatment**  \n   ```python\n   df.replace(0, np.nan, inplace=True)   # for columns where 0 is invalid\n   df.fillna(df.median(), inplace=True)\n   ```\n\n2. **Pipeline & cross\u2011validation**  \n   ```python\n   from sklearn.pipeline import Pipeline\n   from sklearn.model_selection import cross_val_score\n\n   pipe = Pipeline([\n       ('scaler', StandardScaler()),\n       ('clf', LogisticRegression(max_iter=1000, random_state=42))\n   ])\n\n   scores = cross_val_score(pipe, X, y, cv=5, scoring='accuracy')\n   print(\"CV Accuracy:\", scores.mean())\n   ```\n\n3. **Stratified split**  \n   ```python\n   X_train, X_test, y_train, y_test = train_test_split(\n       X, y, test_size=0.2, random_state=42, stratify=y)\n   ```\n\n4. **Class imbalance handling**  \n   ```python\n   model = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n   ```\n\n5. **Additional metrics** \u2013 ROC\u2011AUC, precision\u2011recall curve, or confusion matrix with class labels.  \n   ```python\n   from sklearn.metrics import roc_auc_score, precision_recall_curve\n\n   y_proba = model.predict_proba(X_test_scaled)[:,1]\n   auc = roc_auc_score(y_test, y_proba)\n   print(\"AUC:\", auc)\n   ```\n\n6. **Documentation** \u2013 Add a markdown cell at the top summarizing the goal, dataset, and key findings.\n\n7. **Code reuse** \u2013 Wrap the training and evaluation logic in a function so you can easily experiment with different models.\n\n---\n\n### Quick \u201ccheat\u2011sheet\u201d of changes\n\n```python\n# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report, roc_auc_score\n\n# 1. Load data\ndf = pd.read_csv(Path('diabetes.csv'))\n\n# 2. Clean missing values\ndf.replace(0, np.nan, inplace=True)\ndf.fillna(df.median(), inplace=True)\n\n# 3. Rename columns\ndf.rename(columns={\n    'Pregnancies': 'Pregnant',\n    'BloodPressure': 'BP',\n    'SkinThickness': 'Skin',\n    'DiabetesPedigreeFunction': 'Pedigree'\n}, inplace=True)\n\n# 4. Feature/target split\nfeature_cols = ['Pregnant', 'BP', 'Insulin', 'BMI', 'Pedigree', 'Age']\nX = df[feature_cols]\ny = df['Outcome']\n\n# 5. Train/test split (stratified)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y)\n\n# 6. Pipeline\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n])\n\n# 7. Cross\u2011validation\ncv_scores = cross_val_score(pipe, X, y, cv=5, scoring='accuracy')\nprint(f\"5\u2011fold CV Accuracy: {cv_scores.mean():.4f}\")\n\n# 8. Fit on training data\npipe.fit(X_train, y_train)\n\n# 9. Predict & evaluate\ny_pred = pipe.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"F1\u2011score:\", f1_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred, target_names=['without diabetes', 'with diabetes']))\n\n# 10. Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.tight_layout()\nplt.show()\n\n# 11. ROC\u2011AUC\ny_proba = pipe.predict_proba(X_test)[:,1]\nprint(\"ROC\u2011AUC:\", roc_auc_score(y_test, y_proba))\n```\n\n---\n\n### Final words\n\nYou\u2019ve demonstrated a clear understanding of the machine\u2011learning workflow: data loading \u2192 preprocessing \u2192 modeling \u2192 evaluation. By adding a few best\u2011practice touches\u2014especially around missing\u2011value handling, stratified splits, pipelines, and reproducibility\u2014you\u2019ll make your notebook more robust, easier to maintain, and ready for a real\u2011world project. Keep experimenting with different models (e.g., RandomForest, XGBoost) and feature engineering; the more you iterate, the deeper your insights will become. Great work, and keep coding!",
        "graded_at": "2025-11-20T23:15:47.035707+00:00",
        "graded_by": "ai",
        "test_results": {
          "tests_run": false,
          "message": "No test cases available for this assignment"
        }
      }
    }
  }
}